<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://flemingchase.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://flemingchase.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-01-08T20:05:38+00:00</updated><id>https://flemingchase.github.io/feed.xml</id><title type="html">blank</title><subtitle>This is my personal website! It includes things written by me and relevant professional information. </subtitle><entry><title type="html">Conditional expectation with respect to von-Neumann subalgebras</title><link href="https://flemingchase.github.io/blog/2025/conditional-expectation/" rel="alternate" type="text/html" title="Conditional expectation with respect to von-Neumann subalgebras"/><published>2025-01-07T16:00:00+00:00</published><updated>2025-01-07T16:00:00+00:00</updated><id>https://flemingchase.github.io/blog/2025/conditional-expectation</id><content type="html" xml:base="https://flemingchase.github.io/blog/2025/conditional-expectation/"><![CDATA[<p>This post is inspired by this <a href="https://www.jstor.org/stable/25449122">paper</a> by D. Shlyakhtenko (<a href="https://arxiv.org/pdf/math/0510103"> arxiv link to similar result</a>). While reading this paper, I came across some notation in formula in the statement of lemma 4 that I was unfamiliar with</p> \[f\left( \sum_{i=1}^{N+1} X^{(k)}_{i} : \left\{\sum_{i=1}^{N+1}X^{(r)}_{i}\right\}_{r\neq k}\right) = E_{W^{\star}\left(\left\{\sum_{i=1}^{N+1} X^{(r)}_{i}\right\}_{r=1}^{n}\right)}f\left(\sum_{i\neq j}X_{i}^{(k)} : \left\{\sum_{i \neq j} X_{i}^{(r)}\right\}_{r \neq k}, \left\{X_{j}^{r}\right\}_{r=1}^{n}\right)\] <p>In particular, what is this</p> \[E_{W^{\star}\left(\left\{\sum_{i=1}^{N+1} X^{(r)}_{i}\right\}_{r=1}^{n}\right)}\] <p>and what does this mean? My advisor told me that the \(E\) represents a conditional expectation and the \(W^\star\) represents a von-Neumann subalgebra. So, the goal of this post is to unpack this notation entirely.</p> <p>The standard definition of conditional expectation takes place in the arena of some probability space \((\Omega, \Sigma, \mathcal{P})\) with two random variables \(X\) and \(Y\). Then, intuitively, we can construct a new random variable \(Z = E[X|Y]\). This new random variable gives us the expectation of \(X\) given \(Y\) occurs. A simple example of this given on the <a href="https://en.wikipedia.org/wiki/Conditional_expectation#Example_1:_Dice_rolling"> wikipedia page</a> for conditional expectation. However,</p>]]></content><author><name></name></author><category term="math"/><summary type="html"><![CDATA[some notes on a paper by D. Shlyakhtenko]]></summary></entry><entry><title type="html">Properties of the score function</title><link href="https://flemingchase.github.io/blog/2025/score-function/" rel="alternate" type="text/html" title="Properties of the score function"/><published>2025-01-07T16:00:00+00:00</published><updated>2025-01-07T16:00:00+00:00</updated><id>https://flemingchase.github.io/blog/2025/score-function</id><content type="html" xml:base="https://flemingchase.github.io/blog/2025/score-function/"><![CDATA[<p>In the proof of theorem 5 in this <a href="huh">this</a> paper by Syhlakhtenko, my advisor and I came across something curious. The first three equalities are</p> \[\begin{aligned} f(Z^{(k)}_{N+1} : {Z^{(r)}_{N+1}}_{r\neq k}) &amp;= (N+1)^{1/2}f\left( \sum_{j=1}^{N+1} X^{(k)}_{j} : {Z_{N+1}^{(r)}}_{r\neq k} \right) \\ &amp;= E_{M}\left((N+1)^{1/2}f\left(\sum_{i\neq j} X^{(k)}_{i} : \left\{ \sum_{i\neq j} X^{(r)}_{i}\right\}_{r\neq k}, \{X^{(r)}_{j}\}^{n}_{r=1} \right)\right)\\ &amp;= (N+1)^{1/2}E_{M}f\left(\sum_{i\neq j} X^{(k)}_{i} : \left\{\sum_{i\neq j} X^{(r)}_{i}\right\}_{r \neq k}\right) \end{aligned}\] <p>These were not immediate to us. So, we will justify each equality symbol in this proof. Before, we begin, let’s state the setting.</p> <h3 id="background">Background</h3> <p>Let \(\{(X^{(1)}_{j}, \ldots, X^{(n)}_{j})\}_{j &lt; \omega}\) be a sequence of \(n\)-tuples of random variables where each \(X_{i}^{(k)}\) lives inside some fixed operator algebra \(A\). Moreoever, assume that for each \(i,k &lt; \omega, i\neq k\) the \(n\)-tuples \((X^{(1)}_{i},\ldots,X^{n}_{i})\) and \((X^{(1)}_{k},\ldots,X^{(n)}_{k})\) are independent and identically distrubted with finite second moments.</p> <p>We can visualize this as placing these random variables in an \(n \times \omega\) matrix:</p> \[\begin{bmatrix} X^{(1)}_{1} &amp; X^{(2)}_{1} &amp; \ldots &amp; X^{(n)}_{1} \\ X^{(1)}_{2} &amp; X^{(2)}_{2} &amp; \ldots &amp; X^{(n)}_{2} \\ \vdots &amp; \vdots &amp; \vdots &amp; \vdots \\ X^{(1)}_{k} &amp; X^{(2)}_{k} &amp; \ldots &amp; X^{(n)}_{k} \\ \vdots &amp; \vdots &amp; \vdots &amp; \vdots \end{bmatrix}\] <p>where each row, considered as an \(n\)-tuple is independent and identitcally distrubted from any other row.</p> <p>Next, we define new elements, the \(Z^{(k)}_{N}\). For every \(1 \leq k \leq n\) and $N &lt; \omega$, let</p> \[Z^{(k)}_{N} = \frac{1}{\sqrt{N}}\sum_{i=1}^{N} X^{(k)}_{i} = \frac{X_{1}^{(k)} + X^{(k)}_{2} + \ldots + X^{(k)}_{N}}{\sqrt{N}}\] <p>So, we can construct a new \(n\times \omega\) matrix</p> \[\begin{bmatrix} Z^{(1)}_{1} &amp; Z^{(2)}_{1} &amp; \ldots &amp; Z^{(n)}_{1} \\ Z^{(1)}_{2} &amp; Z^{(2)}_{2} &amp; \ldots &amp; Z^{(n)}_{2} \\ \vdots &amp; \vdots &amp; \vdots &amp; \vdots \\ Z^{(1)}_{k} &amp; Z^{(2)}_{k} &amp; \ldots &amp; Z^{(n)}_{k} \\ \vdots &amp; \vdots &amp; \vdots &amp; \vdots \end{bmatrix}\] <p>where the \((N,k)\)th entry in the above matrix, \(Z^{(k)}_{N}\) is the statistically normalized sum of the first \(N\) elements in the \(k\)th row of the original matrix.</p> <p>Next, we define this score function \(f\). For a fixed \(n\)-tuple of random variables \((X^{(1)}, X^{(2)}, \ldots, X^{(n)})\), define the score function with respect the \(j\)th variable as</p> \[f_{j} = f(X^{(j)}: X^{(1)}, \ldots, \hat{X}^{(j)}, \ldots, X^{(n)}) = -\frac{\partial p/\partial x_{j}}{p}\] <p>Where \(p\) is the joint densitiy function of the random variables \(X^{(1)}, \ldots X^{(n)}\).</p> <p>Finally, we restate two important lemmas from the paper.</p> <p><strong>Lemma 3</strong> <em>Assume that \(X\) is independent from \(Y^{(1)}, \ldots, Y^{(n)}\). Then one has the equality</em></p> <p>\(f(Y^{(j)}:Y^{(1)}, \ldots, \hat{Y}^{(j)}, \ldots Y^{(n)}, X) = f(Y^{(j)}: Y^{(1)}, \ldots, \hat{Y}^{(j)},\ldots, Y^{(n)})\).</p> <p>Essentially, this lemma states that score function is unchanged for an \(n\)-tuple of random variables if you consider an \(n+1\)-tuple where the random variable appended is independent from all previous random variables. The proof is immediate from definition of joint density and the product rule.</p> <p><strong>Lemma 4</strong> <em>Assume that \(\{X^{(k)}_{j}\}, 1\leq k \leq n, j= 1, 2,3, \ldots\) are random variables. Then, for each \(j = 1,2,3,\ldots, N+1\) and each \(1 \leq k \leq n\) we have the equality</em></p> \[f\left( \sum_{i=1}^{N+1} X^{(k)}_{i} : \left\{ \sum_{i=1}^{N+1} X^{(r)}_{i} \right\}_{r\neq k} \right) = E_{M} f\left( \sum_{i\neq j}^{N+1} X^{(k)}_{i} : \left\{ \sum_{i\neq j}^{N+1} X^{(r)}_{i} \right\}_{r\neq k}, \{X^{(r)}_{j}\}_{r=1}^{n}\right)\] <p><em>where \(M = W^{\star}\left( \left\{ \sum_{i=1}^{N+1} X^{(r)}_{i} \right\}_{r=1}^{n} \right)\).</em></p> <p>Lemma \(4\) at first glance seems very difficult to parse. So, let’s unpack it. Fix some \(1 \leq j &lt; N+1\). Let’s do some variable replacement. Let</p> \[Y^{(k)} = \sum_{i=1}^{N+1} X^{(k)}_{i} \text{and}\] <p>and</p> \[Y^{(k)\prime} = \sum_{i\neq j}^{N+1} X^{(k)}_{i}\] <p>With this replacement, we see that \(Y^{(k)}\) is just the sum of the first \(N+1\) random variables in the \(k\)th column, and \(Y^{(k)\prime}\) is the sum of the first \(N+1\) random variables in the \(k\)th column excluding the \(j\)th row entry.</p> <p>We may now restate lemma \(4\) as</p> \[f(Y^{(k)} : Y^{(1)}, \ldots, \hat{Y}^{(k)}, \ldots, Y^{(n)}) = E_{M} f(Y^{(k)\prime} : Y^{(1)\prime}, \ldots, \hat{Y}^{(k)\prime}, \ldots, Y^{(k)\prime}, X^{(1)}_{j}, \ldots, X^{(n)}_{j})\] <p>where \(M\) is the smallest subalgebra containing all the \(Y^{(k)}\)s.</p> <p>Then, in words, lemma 4 then says that the score of \(Y^{(k)}\) with respect to the joint distribution of \(Y^{(1)},\ldots, Y^{(n)}\) is equal to the conditional expectation with respect to the smallest subalgebra containing all the \(Y^{(k)}\)s of the score of \(Y^{(k)\prime}\) with respect to the joint distrubtion of \(Y^{(1)\prime},\ldots, Y^{(n)\prime}, X^{(1)}_{j}, \ldots, X^{(n)}_{j}\) (see my previous <a href="/blog/2025/conditional-expectation/"> blog</a> on conditional expectation in this context).</p> <p>We are now ready to justify each of the three equalities. So, let’s state the assumptions of lemma \(5\).</p> <p><strong>Lemma 5</strong> Let \((X^{(1)}_{j}, \ldots, X^{(n)}_{j}), j=1, 2, 3, \ldots\) be a sequence of \(n\)-tuples so that these tuples of independent and identitically distrubuted and have finite second moments. Let $$Z^{(k)}<em>{N} = X^{(k)}</em>{1} + \ldots</p> <ul> <li>X^{(k)}_{N}/\sqrt{N}$$.</li> </ul> <h3 id="first-equality">First equality</h3> <p>Let us compute the left and right hand side of the equality. Let \(f_{k}\) be the density of the random variable \(X^{(k)}_{1}\). Note that by the assumptions, that \(f_{k}\) is also the density of \(X^{(k)}_{j}\) for all \(j=1, 2, 3, \ldots\). Then, the density of \(Z^{(k)}_{N+1}\) is given by</p> \[p_{N+1}^{(k)}(z_{k}) = (N+1)^{1/2}(f_{1}^{\star(N+1)})((N+1)^{1/2}z) = (N+1)^{1/2}(f_{1} \star \ldots \star f_{1})((N+1)^{1/2}z_{k})\] <p>where the convolution is taken \(N+1\). See <a href="https://en.wikipedia.org/wiki/Convolution_of_probability_distributions">this</a> and <a href="https://math.stackexchange.com/a/1689639/220841&gt;">this</a> for justification.</p> <p>Thus, the joint density function for \(Z_{N+1}^{(1)}, \ldots, Z^{(n)}_{N+1}\) is given by</p> \[p(z_{1},\ldots, z_{n}) = \prod_{i=1}^{n} p_{N+1}^{(i)}(z_{i})\] <p>as each column is independent.</p> <p><strong>Left hand side</strong></p> <p>Therefore, we can compute the score with respect to \(Z_{N+1}^{(k)}\) as</p> \[\begin{aligned} f(Z_{N+1}^{(k)}: Z^{(1)}, \ldots, \hat{Z}^{(k)}_{N+1}, \ldots, Z^{(k)}_{N+1}) &amp;= - \frac{\partial p / \partial z_{j}}{p}\\ &amp;= -\frac{1}{\prod_{i=1}^{n} p_{N+1}^{(i)}(z_{i})} \frac{\partial}{\partial z_{k}} \prod_{i=1}^{n} p_{N+1}^{(i)}(z_{i}) \\ &amp;= -\frac{1}{\prod_{i=1}^{n} p_{N+1}^{(i)}} \frac{\partial p_{N+1}^{(k)}(z_{k})}{\partial z_{k}} \prod_{i\neq k} p_{N+1}^{(i)} \\ &amp;= -\frac{\partial p_{N+1}^{(k)}(z_{k}) / \partial z_{k}}{p_{N+1}^{(k)}(z_{k})} \\ &amp;=-\frac{(N+1)^{1/2}(N+1)^{1/2}f_{k}^{\star(N+1)\prime}((N+1)^{1/2}z_{k})}{(N+1)^{1/2}f_{k}^{\star(N+1)}((N+1)^{1/2}z_{k})}\\ &amp;= - (N+1)^{1/2} \frac{f^{\star(N+1)\prime}}{f^{\star(N+1)}}((N+1)^{1/2}z_{k}) \end{aligned}\] <p><strong>Right hand side</strong></p> <p>We note compute the score of \((N+1)^{1/2}Z_{N+1}^{(k)}=\sum_{j=1}^{N+1} X^{(k)}_{j}\) with respect to the joint density of \(Z_{N+1}^{(1)}, \ldots, (N+1)^{1/2}Z_{N+1}^{(k)}, \ldots, Z^{(k)}_{N+1}\)</p> <p>We note that the density of \((N+1)^{1/2}Z_{N+1}^{(k)}\) is simply</p> \[\rho_{N+1}^{(k)}(z_{k}) = (f_{k}^{\star(N+1)})(z_{k})\] <p>Then the joint density function is</p> \[p = \rho_{N+1}^{(k)}(z_{k})\prod_{i\neq k} p_{N+1}^{(i)}(z_{i})\] <p>Then we compute the score as follows</p> \[\begin{aligned} f((N+1)^{1/2}Z_{N+1}^{(k)} : Z_{N+1}^{(1)}, \ldots, (N+1)^{1/2}\hat{Z}^{(k)}_{N+1}, \ldots, Z^{(n)}_{N+1}) &amp;= - \frac{\partial p / \partial z_{k}}{p} \\ &amp;= - \frac{1}{\rho_{N+1}^{(k)}(z_{k})\prod_{i\neq k} p_{N+1}^{(i)}(z_{i})} \frac{\partial}{\partial z_{k}} \rho_{N+1}^{(k)}(z_{k}) \prod_{i\neq k} p_{N+1}^{(i)}(z_{i})\\ &amp;= -\frac{f^{\star(N+1)\prime}(z_{k})}{f^{\star(N+1}(z_{k})} \end{aligned}\] <h3 id="second-equality">Second equality</h3> <h3 id="third-equality">Third equality</h3>]]></content><author><name></name></author><category term="math"/><summary type="html"><![CDATA[some more notes on a paper by D. Shlyakhtenko]]></summary></entry></feed>